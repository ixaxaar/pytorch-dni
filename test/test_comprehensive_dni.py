#!/usr/bin/env python3
"""
Comprehensive DNI test following the original paper:
"Decoupled Neural Interfaces using Synthetic Gradients" by Jaderberg et al.

This test validates the key concepts:
1. Synthetic gradient generation
2. Decoupled learning with Œª-mixing
3. Forward-pass independence 
4. DNI network learning
"""

import sys
import os
sys.path.insert(0, '.')

import torch
import torch.nn as nn
import torch.nn.functional as F
from dni import DNI, LinearDNI, RNNDNI
import numpy as np

def test_basic_dni_concepts():
    """Test basic DNI functionality according to paper"""
    print("=" * 60)
    print("Testing Basic DNI Concepts (Jaderberg et al., 2017)")
    print("=" * 60)
    
    # Create a simple MLP
    net = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.Linear(20, 15),
        nn.ReLU(), 
        nn.Linear(15, 5)
    )
    
    # Test different Œª values as described in paper
    lambda_values = [-1, 0.0, 0.5, 1.0]  # -1 = DNI + BP, 0 = pure DNI, 0.5 = mixed, 1.0 = pure BP
    
    for lam in lambda_values:
        print(f"\nTesting Œª = {lam}")
        
        # Wrap with DNI
        dni_net = DNI(
            net,
            hidden_size=16,
            Œª=lam,
            recursive=True,
            skip_last_layer=True  # Don't apply DNI to output layer as per paper
        )
        
        dni_net.train()
        
        # Forward pass
        input_data = torch.randn(8, 10, requires_grad=True)
        output = dni_net(input_data)
        
        # Compute loss
        target = torch.randn(8, 5)
        loss = F.mse_loss(output, target)
        
        # Backward pass
        loss.backward()
        
        # Execute DNI optimizer steps
        dni_net.step_optimizers()
        
        print(f"  ‚úì Œª={lam}: Forward/backward successful, output shape: {output.shape}")
    
    print("‚úì Basic DNI concepts test passed!")

def test_rnn_dni():
    """Test RNN with DNI as demonstrated in paper"""
    print("\n" + "=" * 60)
    print("Testing RNN with DNI")
    print("=" * 60)
    
    # Create RNN
    rnn = nn.LSTM(input_size=32, hidden_size=64, num_layers=2, batch_first=True)
    
    # Wrap with DNI using RNNDNI
    dni_rnn = DNI(
        rnn,
        dni_network=RNNDNI,
        hidden_size=32,
        Œª=0.0,  # Pure synthetic gradients
        recursive=False,
        format_fn=lambda x: x[0]  # Extract output from (output, hidden) tuple
    )
    
    dni_rnn.train()
    
    # Test data (batch_size, seq_len, input_size)
    batch_size, seq_len, input_size = 4, 10, 32
    input_data = torch.randn(batch_size, seq_len, input_size, requires_grad=True)
    
    # Forward pass
    output, hidden = dni_rnn(input_data)
    
    # Simple loss
    loss = output.sum()
    
    # Backward pass
    loss.backward()
    
    # Execute DNI optimizer steps
    dni_rnn.step_optimizers()
    
    print(f"‚úì RNN DNI test passed! Output shape: {output.shape}")

def test_dni_learning():
    """Test that DNI networks actually learn to predict gradients"""
    print("\n" + "=" * 60)
    print("Testing DNI Learning Capability")
    print("=" * 60)
    
    # Simple network for testing
    net = nn.Sequential(
        nn.Linear(5, 10),
        nn.ReLU(),
        nn.Linear(10, 3)
    )
    
    # Wrap with DNI
    dni_net = DNI(
        net,
        hidden_size=8,
        Œª=0.0,  # Pure synthetic gradients
        recursive=True,
        skip_last_layer=True
    )
    
    dni_net.train()
    
    # Training loop to test DNI learning
    initial_losses = []
    final_losses = []
    
    for epoch in range(10):
        # Generate random data
        input_data = torch.randn(16, 5, requires_grad=True)
        target = torch.randn(16, 3)
        
        # Forward pass
        output = dni_net(input_data)
        loss = F.mse_loss(output, target)
        
        if epoch == 0:
            initial_losses.append(loss.item())
        elif epoch == 9:
            final_losses.append(loss.item())
            
        # Backward pass
        loss.backward()
        
        # Execute DNI optimizer steps
        dni_net.step_optimizers()
    
    print(f"‚úì DNI learning test completed")
    print(f"  Initial loss: {np.mean(initial_losses):.4f}")
    print(f"  Final loss: {np.mean(final_losses):.4f}")

def test_decoupling_benefit():
    """Test the computational benefits of decoupling"""
    print("\n" + "=" * 60) 
    print("Testing Decoupling Benefits")
    print("=" * 60)
    
    # Create a deeper network to demonstrate decoupling
    net = nn.Sequential(
        nn.Linear(20, 50),
        nn.ReLU(),
        nn.Linear(50, 40),
        nn.ReLU(),
        nn.Linear(40, 30),
        nn.ReLU(),
        nn.Linear(30, 10)
    )
    
    # Test with different configurations
    configs = [
        {"Œª": 1.0, "name": "Standard Backprop"},
        {"Œª": 0.0, "name": "Pure DNI"},
        {"Œª": 0.5, "name": "Mixed (Œª=0.5)"}
    ]
    
    for config in configs:
        dni_net = DNI(
            net,
            hidden_size=25,
            Œª=config["Œª"],
            recursive=True,
            skip_last_layer=True
        )
        
        dni_net.train()
        
        input_data = torch.randn(10, 20, requires_grad=True)
        output = dni_net(input_data)
        loss = output.sum()
        
        loss.backward()
        dni_net.step_optimizers()
        
        print(f"‚úì {config['name']}: Successful")
    
    print("‚úì Decoupling benefits test passed!")

if __name__ == "__main__":
    try:
        test_basic_dni_concepts()
        test_rnn_dni()
        test_dni_learning()
        test_decoupling_benefit()
        
        print("\n" + "üéâ" * 20)
        print("ALL DNI TESTS PASSED!")
        print("Implementation follows Jaderberg et al. (2017) paper")
        print("üéâ" * 20)
        
    except Exception as e:
        print(f"\n‚ùå Error during testing: {e}")
        import traceback
        traceback.print_exc()